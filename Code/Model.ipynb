{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623fc9d4",
   "metadata": {},
   "source": [
    "<h1><center> Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f9a109",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6eaca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec parameters\n",
    "W2V_SIZE = 300 \n",
    "W2V_WINDOW = 7  \n",
    "W2V_MIN_COUNT = 10 \n",
    "W2V_WORKERS=8\n",
    "W2V_EPOCH = 32 \n",
    "\n",
    "# Tokenizer parameters\n",
    "SEQUENCE_LENGTH = 300\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "854a1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"Sentiment\",\"Text\"]\n",
    "data = pd.read_csv(r'D:\\CSC590_Design_Project\\Data\\data.csv',names = col_names,encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8219fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def process_text(text,remove_stops = False, stem = False):\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove url links\n",
    "    text = re.sub(\"@[\\w]*\",'',text) # remove \"@user\"\n",
    "    text = re.sub('[^a-zA-Z]',' ',text) # leave only characters\n",
    "    words =[]\n",
    "    for word in text.split():\n",
    "        if not remove_stops or word not in stops:\n",
    "            if not stem:\n",
    "                words.append(word)\n",
    "            else:\n",
    "                words.append(stemmer.stem(word))\n",
    "    return words    \n",
    "\n",
    "data['Text'] = data['Text'].apply(lambda x: process_text(x,remove_stops = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "befa3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows = round(len(data.index)*0.6)\n",
    "val_rows = round(len(data)*0.2)\n",
    "test_rows = len(data.index)-(val_rows+train_rows)\n",
    "\n",
    "train=data.iloc[:train_rows]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "val = data.iloc[train_rows:train_rows+val_rows]\n",
    "val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test = data.iloc[train_rows+val_rows:]\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68812a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "w2v_model = word2vec.Word2Vec(size=W2V_SIZE, \n",
    "                              window=W2V_WINDOW, \n",
    "                              min_count=W2V_MIN_COUNT, \n",
    "                              workers=W2V_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c2bde",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "train_sentences = train['Text'].tolist()\n",
    "w2v_model.build_vocab(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5454dc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "W2V_EPOCH = 32 \n",
    "w2v_model.train(train_sentences, \n",
    "                total_examples=len(train_sentences), \n",
    "                epochs=W2V_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a31c1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "w2v_model.save('D:\\CSC590_Design_Project\\Model\\w2v_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26b5f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-20 18:21:46,573 : INFO : loading Word2Vec object from D:\\CSC590_Design_Project\\Model\\w2v_model.model\n",
      "2021-05-20 18:21:46,940 : INFO : loading wv recursively from D:\\CSC590_Design_Project\\Model\\w2v_model.model.wv.* with mmap=None\n",
      "2021-05-20 18:21:46,940 : INFO : setting ignored attribute vectors_norm to None\n",
      "2021-05-20 18:21:46,941 : INFO : loading vocabulary recursively from D:\\CSC590_Design_Project\\Model\\w2v_model.model.vocabulary.* with mmap=None\n",
      "2021-05-20 18:21:46,941 : INFO : loading trainables recursively from D:\\CSC590_Design_Project\\Model\\w2v_model.model.trainables.* with mmap=None\n",
      "2021-05-20 18:21:46,942 : INFO : setting ignored attribute cum_table to None\n",
      "2021-05-20 18:21:46,942 : INFO : loaded D:\\CSC590_Design_Project\\Model\\w2v_model.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model = word2vec.Word2Vec.load(\"D:\\CSC590_Design_Project\\Model\\w2v_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "854483da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "772e6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train['Text'])\n",
    "vocab_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef90a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e0d2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(tokenizer.texts_to_sequences(train['Text']), \n",
    "                        maxlen=SEQUENCE_LENGTH)\n",
    "x_val = pad_sequences(tokenizer.texts_to_sequences(val['Text']), \n",
    "                      maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "242ac2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f53bef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838664a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train['Sentiment'].tolist())\n",
    "\n",
    "y_train = encoder.transform(train['Sentiment'].tolist())\n",
    "y_val = encoder.transform(val['Sentiment'].tolist())\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_val = y_val.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488e705",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "  if word in w2v_model.wv:\n",
    "    embedding_matrix[i] = w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35bfb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, \n",
    "                            W2V_SIZE, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length=SEQUENCE_LENGTH, \n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c05fbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52e138",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n",
    "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(x_val,y_val),\n",
    "                    verbose=1, \n",
    "                    callbacks=callbacks)\n",
    "model.save('D:\\CSC590_Design_Project\\Model\\model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb33635",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('D:\\CSC590_Design_Project\\Model\\model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a8b99",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "confidence=[]\n",
    "def predict(tweet):\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([tweet]), maxlen=SEQUENCE_LENGTH)\n",
    "    score = model.predict([x_test])[0]\n",
    "    confidence.append(abs(score-0.5)*200)\n",
    "    return 1 if score > 0.5 else 0\n",
    "\n",
    "test['Result'] = test['Text'].apply(lambda x:predict(x))\n",
    "test['Confidence'] = confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f7e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
